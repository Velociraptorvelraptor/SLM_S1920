{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2 - Classifiers evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy** - provides numerical data structures and required utilities (linear algebra tool) https://numpy.org/\n",
    "\n",
    "**Pandas** - Python DataFrames + reading/writing datasets https://pandas.pydata.org/\n",
    "\n",
    "**Matplotlib** - plotting https://matplotlib.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-white') #changing plot style\n",
    "plt.rcParams['figure.dpi']=90.0 #size of figures\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-learn**\n",
    "\n",
    "Package for machine learning/statistics. Implements many algorithms used in machine learning, utility functions and analysis tools (https://scikit-learn.org/stable/user_guide.html)\n",
    "\n",
    "Few characteristics from [scikit-learn main page](https://scikit-learn.org/stable/index.html):\n",
    "\n",
    "- Simple and efficient tools for predictive data analysis\n",
    "- Accessible to everybody, and reusable in various contexts\n",
    "- Built on NumPy, SciPy, and matplotlib\n",
    "- Open source, commercially usable - BSD license\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Splitting dataset into train and validation subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data loading & pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET=pd.read_fwf(\"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data-numeric\", header = None)\n",
    "DATA_SET.rename(columns = {24: \"target\"}, inplace=True)\n",
    "DATA_SET['target'] = 2 - DATA_SET['target'] #recoding target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#One-time version\n",
    "#random 80% of indexes values\n",
    "rand = random.sample(range(0,len(DATA_SET)-1),int(0.8*len(DATA_SET)))\n",
    "train = DATA_SET.iloc[rand]\n",
    "val = DATA_SET.drop(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reuseable function\n",
    "def split_data_set(dataset, training_fraction):\n",
    "    n = len(dataset)\n",
    "    rand = random.sample(range(0,n-1),int(0.8*n))\n",
    "    train = dataset.iloc[rand]\n",
    "    val = dataset.drop(rand)\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time train_set, val_set = split_data_set(DATA_SET, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Or use function from imported package - train_test_split() from sklearn\n",
    "X = DATA_SET.drop(['target'],axis=1)\n",
    "y = DATA_SET['target']\n",
    "%time X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape, X_test.shape)\n",
    "print (y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is random sampling the best approach? What if one class has many more records than the other?\n",
    "Imbalanced data may lead to poor model which may have good overall performance metrics e.g. accuracy.\n",
    "\n",
    "There are several approaches to tackle the issue:\n",
    "- undersampling, \n",
    "- oversampling, \n",
    "- cost-based performance,\n",
    "- algorithmic approches e.g.SMOTE\n",
    "\n",
    "Quick reading with code samples:\n",
    "[Article on imbalanced data in Python](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost-based approach in model assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff_analysis(y_test, y_test_hat, cost_matrix = np.array([[0,0],[0,0]])):\n",
    "    cutoff_range = np.arange(0, 1.0, 0.01)\n",
    "    i = 0\n",
    "    if np.sum(cost_matrix) == 0:\n",
    "        acc = [0 for x in range(len(cutoff_range))]\n",
    "        for cutoff in cutoff_range:\n",
    "            y_test_hat_bin = np.where(y_test_hat >= cutoff, 1, 0)\n",
    "            conf_mat = confusion_matrix(y_test, y_test_hat_bin)\n",
    "            acc[i] = np.sum(np.diag(conf_mat)) / np.sum(conf_mat)\n",
    "            i = i + 1\n",
    "        return acc\n",
    "    else:\n",
    "        cost = [0 for x in range(len(cutoff_range))]\n",
    "        for cutoff in cutoff_range:\n",
    "            y_test_hat_bin = np.where(y_test_hat >= cutoff, 1, 0)\n",
    "            conf_mat = confusion_matrix(y_test, y_test_hat_bin)\n",
    "            conf_const_mat = np.multiply(conf_mat, cost_matrix)\n",
    "            cost[i] = conf_const_mat.sum() / len(y_test)\n",
    "            i += 1\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building logistic regression model**\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png\" align=\"left\">\n",
    "\n",
    "https://en.wikipedia.org/wiki/Logistic_regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "LR_L1 = model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_L1.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Measuring model performance (cost-based) for different cutoof thresholds and with or without validation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On validation data\n",
    "score_val = LR_L1.predict_proba(X_test)[:,1]\n",
    "#On training data\n",
    "score_train = LR_L1.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_val = cutoff_analysis(y_test, score_val, cost_matrix = np.array([[0,5],[1,0]]))\n",
    "cost_train = cutoff_analysis(y_train, score_train, cost_matrix = np.array([[0,5],[1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Cutoff point\")\n",
    "plt.ylabel(\"Cost per client\")\n",
    "plt.title(\"Cost vs. cut-off\")\n",
    "\n",
    "plt.plot(np.arange(0, 1.0, 0.01), cost_val)\n",
    "plt.plot(np.arange(0, 1.0, 0.01), cost_train, linestyle = \":\")\n",
    "plt.plot([0, 1], [min(cost_val), min(cost_val)], color = 'gray', label = \"Min Cost Val = \" + str(round(min(cost_val),3)) + \" for k = \" + str(round(np.arange(0, 1.0, 0.01)[cost_val.index(min(cost_val))],2)))\n",
    "plt.plot([0, 1], [min(cost_train), min(cost_train)], color = 'gray', linestyle = \":\", label = \"Min Cost Train = \" + str(round(min(cost_train),3)) + \" for k = \" + str(np.arange(0, 1.0, 0.01)[cost_train.index(min(cost_train))]))\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we got lower cost for predictions on training set, but model may **overfit**.\n",
    "\n",
    "Splitting data into training and validation set is done to avoid overfitting and to assess performance in more realistic situation (model will predict outcome on data it has never seen before).\n",
    "\n",
    "Related to Bias vs. Variance Tradeoff (Underfitting vs. Overfitting)\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#The-Bias-variance-trade-off\n",
    "\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-validation-curve.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assessing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat\", sep = \" \", header = None)\n",
    "dataset.columns = [\"V\" + str(i) for i in range(0,15)]\n",
    "dataset.rename(columns = {\"V14\": \"class\"}, inplace=True)\n",
    "\n",
    "dataset['V3'] = np.where(dataset['V3'] == 1, 0, 1)\n",
    "dataset['V11'] = np.where(dataset['V11'] == 1, 0, 1)\n",
    "dataset['V13'] = np.log(dataset['V13'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data split using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fraction = 0.8\n",
    "X = dataset.iloc[:,0:14]\n",
    "y = dataset.iloc[:,14]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 1-training_fraction, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression estimation with L1 regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_log_reg = LogisticRegression(random_state=1, solver='liblinear', penalty='l1')\n",
    "model_log_reg_fit = model_log_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L1 regularization allows for feature selection**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Lasso_(statistics)\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6f7b5020a85afe0ca7e4ac1bcda7d193bc812617)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log_reg_fit.coef_\n",
    "#First coefficient is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = model_log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**\n",
    "\n",
    "❗ Remember class indicator (0, 1,...) and actual or predicted values may be switched in confusion matrix\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png\" width=400>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1780/1*LQ1YMKBlbDhH9K6Ujz8QTw.jpeg\"  width=400>\n",
    "\n",
    "**Performance measures derived from confusion matrix:**\n",
    "\n",
    "- Accuracy - percentage of correct predictions\n",
    "\n",
    "`ACC = (TP + TN)/(TP + FP + TN + FN)`\n",
    "\n",
    "- Precision - percentage of positive predictions which were actually correct\n",
    "\n",
    "`PREC = TP / (TP + FP)`\n",
    "\n",
    "-  Recall - what percentage of actual positives were predicted correctly\n",
    " (Recall = Sensitivity = Hit rate = True Positive Rate (TPR))\n",
    " \n",
    "`REC = TP / (TP + FN)`\n",
    "\n",
    "- Specificity - what percentage of actual negatives were predicted correctly (Specificity = True Negative Rate)\n",
    "\n",
    "`TNR = TN / (TN + FP)`\n",
    "\n",
    "- F1 Score - traditional F-measure or balanced F-score (F1 score) is the harmonic mean of precision and recall\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/1bf179c30b00db201ce1895d88fe2915d58e6bfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm = confusion_matrix(y_test,y_test_hat)\n",
    "confm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is what in that confusion matrix?\n",
    "confusion_matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ACC= (confm[0,0]+confm[1,1])/(confm[0,0]+confm[1,1]+confm[0,1]+confm[1,0])\n",
    "PREC = (confm[1,1])/(confm[1,1]+confm[0,1])\n",
    "REC = (confm[1,1])/(confm[1,1]+confm[1,0])\n",
    "TNR = (confm[0,0])/(confm[0,0]+confm[0,1])\n",
    "F1 = 2*PREC*REC/(PREC+REC)\n",
    "print(\"ACC \",ACC,\"\\nPREC \",PREC,\"\\nREC \",REC,\"\\nTNR \",TNR,\"\\nF1 \",F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sklearn built-in report\n",
    "print(classification_report(y_test,y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding optimal cut-off based on ACC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = model_log_reg.predict_proba(X_test)[:,1]\n",
    "acc_k = cutoff_analysis(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Cutoff point\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. cut-off\")\n",
    "\n",
    "plt.plot(np.arange(0, 1.0, 0.01), acc_k)\n",
    "plt.plot([0, 1], [max(acc_k), max(acc_k)], color = 'black', linestyle = \":\", label = \"Max ACC = \" + str(round(max(acc_k),3)) + \" for k = \" + str(np.arange(0, 1.0, 0.01)[acc_k.index(max(acc_k))]))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual analysis of a model\n",
    "\n",
    "**Gain&Lift&ROC curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_lift_roc_plot(y_test, y_test_hat, n, return_chart = \"gain\"):\n",
    "    cutoff_range = np.arange(0, 1.0, 0.01)\n",
    "    rpp = [0 for x in range(len(cutoff_range))]\n",
    "    tpr = [0 for x in range(len(cutoff_range))]\n",
    "    lift = [0 for x in range(len(cutoff_range))]\n",
    "    fpr = [0 for x in range(len(cutoff_range))]\n",
    "    prc = [0 for x in range(len(cutoff_range))]\n",
    "    x0 = sum(y_test) / len(y_test)\n",
    "    i = 0\n",
    "    plt.subplot(2, 2, n)\n",
    "    for cutoff in cutoff_range:\n",
    "        y_test_hat_bin = np.where(y_test_hat >= cutoff, 1, 0)\n",
    "        conf_mat = confusion_matrix(y_test, y_test_hat_bin)\n",
    "        rpp[i] = np.sum(conf_mat[:,1]) / np.sum(conf_mat)\n",
    "        fpr[i] = conf_mat[0,1] / np.sum(conf_mat[0,:])\n",
    "        tpr[i] = conf_mat[1,1] / np.sum(conf_mat[1,:])\n",
    "        prc[i] = conf_mat[1,1] / np.sum(conf_mat[:,1])\n",
    "        lift[i] = tpr[i] / rpp[i]\n",
    "        i = i + 1\n",
    "    if return_chart == \"gain\":\n",
    "#         plt.figure()\n",
    "        plt.xlabel(\"Rate of Positive Predictions\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.plot(rpp, tpr, color=\"orange\", label = \"Model\")\n",
    "        plt.plot([0,1],[0,1], color='grey', linestyle=\"--\", label = \"Random\") # random\n",
    "        plt.plot([0,x0],[0,1], color='navy', linestyle=':', label = \"Wizard\") # wizard\n",
    "        plt.plot([x0,1],[1,1], color='navy', linestyle=':') # wizard\n",
    "        plt.legend(loc = \"lower right\")\n",
    "        plt.title(\"Gain chart\")\n",
    "    elif return_chart == \"lift\":\n",
    "#         plt.figure()\n",
    "        plt.xlabel(\"Rate of Positive Predictions\")\n",
    "        plt.ylabel(\"Lift\")\n",
    "        plt.plot(rpp, lift, color=\"orange\", label = \"Model\")\n",
    "        plt.plot([0,1],[1,1], color='grey', linestyle=\"--\", label = \"Random\") # random\n",
    "        plt.plot([0,x0],[1/x0,1/x0], color='navy', linestyle=':', label = \"Wizard\") # wizard\n",
    "        plt.plot([x0,1],[1/x0,1], color='navy', linestyle=':') # wizard\n",
    "        plt.legend(loc = \"upper right\")\n",
    "        plt.title(\"Lift chart\")\n",
    "    elif return_chart == \"roc\":\n",
    "#         plt.figure()\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.plot(fpr, tpr, color=\"orange\", label = \"Model\")\n",
    "        plt.plot([0,1],[0,1], color='grey', linestyle=\"--\", label = \"Random\") # random\n",
    "        plt.plot([0,0],[0,1], color='navy', linestyle=':', label = \"Wizard\") # wizard\n",
    "        plt.plot([0,1],[1,1], color='navy', linestyle=':') # wizard\n",
    "        plt.legend(loc = \"lower right\")\n",
    "        plt.title(\"ROC chart\")\n",
    "    elif return_chart == \"precision-recall\":\n",
    "#         plt.figure()\n",
    "        plt.xlabel(\"Recall [True Positive Rate]\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.plot(tpr, prc, color=\"orange\", label = \"Model\")\n",
    "        plt.plot([0,1],[1,0], color='grey', linestyle=\"--\", label = \"Random\") # random\n",
    "        plt.plot([0,1],[1,1], color='navy', linestyle=':', label = \"Wizard\") # wizard\n",
    "        plt.plot([1,1],[0,1], color='navy', linestyle=':') # wizard\n",
    "        plt.legend(loc = \"lower left\")\n",
    "        plt.title(\"Precision-recall chart\")\n",
    "    plt.tight_layout(pad=0.4, w_pad=1.0, h_pad=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8, 7]\n",
    "gain_lift_roc_plot(y_test, y_test_hat,1)\n",
    "gain_lift_roc_plot(y_test, y_test_hat,2, \"lift\")\n",
    "gain_lift_roc_plot(y_test, y_test_hat,3, \"roc\")\n",
    "gain_lift_roc_plot(y_test, y_test_hat,4, \"precision-recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC curve + AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_test_hat)\n",
    "auc_roc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"ROC&AUC using sklearn\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', label = \"Random, AUC = 0.5\")\n",
    "plt.plot([0, 0], [0, 1], color='navy', linestyle=':', label = \"Wizard, AUC = 1.0\")\n",
    "plt.plot([0, 1], [1, 1], color='navy', linestyle=':')\n",
    "\n",
    "plt.plot(fpr, tpr, color = 'orange', label='Model, AUC = %0.2f' % auc_roc)\n",
    "\n",
    "plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score-density plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_density_plot(y_test, y_test_hat):\n",
    "    basis_x = np.linspace(-0.2,1.2,1000)\n",
    "    y_test_hat_0 = y_test_hat[y_test == 0]\n",
    "    y_test_hat_1 = y_test_hat[y_test == 1]\n",
    "    wizard_0 = y_test[y_test == 0]\n",
    "    wizard_1 = y_test[y_test == 1]\n",
    "    avg= np.mean(y_test)\n",
    "    ran = [1]\n",
    "    for i in range(0,len(y_test)-1):\n",
    "        ran = ran + [int(np.mean(ran) < avg)]\n",
    "    ran = pd.Series(ran)\n",
    "    random_0 = pd.Series(np.linspace(0,1.0,len(y_test)))[ran==0]\n",
    "    random_1 = pd.Series(np.linspace(0,1.0,len(y_test)))[ran==1]\n",
    "    # Model\n",
    "    kde_0 = KernelDensity(kernel='epanechnikov', bandwidth=0.20)\n",
    "    kde_0.fit(y_test_hat_0[:, None])\n",
    "    prob_0 = np.exp(kde_0.score_samples(basis_x[:, None]))\n",
    "    kde_1 = KernelDensity(kernel='epanechnikov', bandwidth=0.20)\n",
    "    kde_1.fit(y_test_hat_1[:, None])\n",
    "    prob_1 = np.exp(kde_1.score_samples(basis_x[:, None]))\n",
    "    # Wizard\n",
    "    kde_0w = KernelDensity(kernel='epanechnikov', bandwidth=0.20)\n",
    "    kde_0w.fit(wizard_0[:, None])\n",
    "    prob_0w = np.exp(kde_0w.score_samples(basis_x[:, None]))\n",
    "    kde_1w = KernelDensity(kernel='epanechnikov', bandwidth=0.20)\n",
    "    kde_1w.fit(wizard_1[:, None])\n",
    "    prob_1w = np.exp(kde_1w.score_samples(basis_x[:, None]))\n",
    "    # Random\n",
    "    kde_0r = KernelDensity(kernel='epanechnikov', bandwidth=0.20)\n",
    "    kde_0r.fit(random_0[:, None])\n",
    "    prob_0r = np.exp(kde_0r.score_samples(basis_x[:, None]))\n",
    "    kde_1r = KernelDensity(kernel='epanechnikov', bandwidth=0.20)\n",
    "    kde_1r.fit(random_1[:, None])\n",
    "    prob_1r = np.exp(kde_1r.score_samples(basis_x[:, None]))   \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(basis_x, prob_0, alpha=0.5, color = \"orange\", lw=2, label = \"Model\")\n",
    "    plt.plot(basis_x, prob_1, alpha=0.5, color = \"orange\", linestyle=\":\", lw=2)\n",
    "    plt.plot(basis_x, prob_0w, alpha=0.5, color = \"navy\", label = \"Wizard\")\n",
    "    plt.plot(basis_x, prob_1w, alpha=0.5, color = \"navy\", linestyle=\":\")\n",
    "    plt.plot(basis_x, prob_0r, alpha=0.5, color = \"grey\", label = \"Random\")\n",
    "    plt.plot(basis_x, prob_1r, alpha=0.5, color = \"grey\", linestyle=\":\")\n",
    "    plt.title(\"Score density plot\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend(loc=\"upper center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score_density_plot(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Iris dataset from https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv to 'iris' DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code `species` column to have value 1 if iris is from _versicolor_ species and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset to train and validation subsets using `train_test_split` function. Training set should have **75%** of all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build logistic regression (with `LogisticRegression` from `sklearn`) using **Elastic-net** regularization with 0.35 L1 ratio (only one solver supports that, check [here](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))\n",
    "\n",
    "You can read more about **Elastic-net** [here](https://en.wikipedia.org/wiki/Elastic_net_regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make classification report with `classification_report`. What is accuracy of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why accuracy is so low? If you want to know check [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Unsupervised-learning-example:-Iris-dimensionality) below `In[19]`. Plot shows how target classes are distributed in 2D space (which was possible due to dimensionality reduction technique PCA - note that we have 4 predictors (sepal_length/width,petal_length/width) not 2). Remember we merged setosa and virginica species - knowing that look were versicolor is on the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write function \n",
    "\n",
    "`plot_acc_train_vs_val(y_train, y_test, y_train_hat, y_test_hat)` \n",
    "\n",
    "that takes following arguments:\n",
    "\n",
    "- y_train - array of class labels (0 or 1) for training data\n",
    "- y_test - array of class labels (0 or 1) for validation data \n",
    "- y_train_hat - array of probabilities (0 to 1) for class 1 for training data\n",
    "- y_test_hat - array of probabilities (0 to 1) for class 1 for validation data\n",
    "\n",
    "and produce plot like in **Finding optimal cut-off based on ACC** subsection but for both prediction on training and validation data. \n",
    "\n",
    "While creating function you _can_ use code as below:\n",
    "\n",
    "```python\n",
    "    acc_t = cutoff_analysis(y_train, y_train_hat)\n",
    "    acc_v = ...\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Cutoff point\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy vs. cut-off\")\n",
    "\n",
    "    plt.plot(np.arange(0, 1.0, 0.01), acc_t)\n",
    "    plt.plot(... , linestyle = \":\")\n",
    "    plt.plot([0, 1], [max(acc_t), max(acc_t)], color = 'gray', label = \"Max ACC train= \" + str(round(max(acc_t),3)) + \n",
    "             \" for k = \" + str(np.arange(0, 1.0, 0.01)[acc_t.index(max(acc_t))]))\n",
    "    plt.plot(.....................................)\n",
    "    plt.legend();\n",
    "```\n",
    "\n",
    "Then test your new function using (of course after filling placeholders):\n",
    "\n",
    "```python\n",
    "y_train_hat = model. ...\n",
    "y_test_hat = model. ...\n",
    "plot_acc_train_vs_val(y_train, y_test, y_train_hat, y_test_hat)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the plot you may see that accuracy for train and validation sets prediction id quite similar. Shouldn't the accuracy be better on the training set prediction? In this case not necessarily becuase model is **underfitted** - in other words it's biased and may perform better on validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
