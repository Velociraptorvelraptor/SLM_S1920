{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 4 - Decision trees and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(color_codes=True);\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading and pre-processing dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use IMDB 5000 Movies dataset in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./Class4_data.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprehensive data report may be generated using `pandas-profiling` package (see **DatasetReport.html** on repo). Package is quite big and may be unstable on Jupyter in Windows, so we'll skip it during the class. If you are interested in the package please check [Pandas-profiling official docs](https://pandas-profiling.github.io/pandas-profiling/docs/)\n",
    "\n",
    "Code required to generate the report:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import pandas-profiling\n",
    "dataset = pd.read_something()\n",
    "report =dataset.profile_report(sort='None', html={'style':{'full_width': True}}, progress_bar=True)\n",
    "report #to show report in Jupyter\n",
    "report.to_file(output_file=\"DatasetReport.html\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Inspecting columns\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking numeric columns\n",
    "dataset.describe(include = [np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking string columns\n",
    "## Count ,number of uniques and most frequent value of non-numeric\n",
    "dataset.describe(include = ['O']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns with many unique values and imbalanced classes\n",
    "dataset.drop(['color','director_name','actor_2_name','actor_1_name',\n",
    "             'movie_title','actor_3_name','plot_keywords',\n",
    "             'movie_imdb_link','language','country','content_rating'],\n",
    "            axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates\n",
    "print(dataset.shape)\n",
    "dataset.drop_duplicates(inplace = True)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check null values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dropping missing values\n",
    "dataset.dropna(inplace=True)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Splitting genres column values and one-hot encoding it\n",
    "# Getting distinct categories\n",
    "categories = set(dataset.genres.str.split(\"|\").explode())\n",
    "\n",
    "# one-hot encode each movie's classification\n",
    "for cat in categories:\n",
    "    dataset[cat] = dataset.genres.transform(lambda s: int(cat in s))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop genres column\n",
    "dataset.drop('genres',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing outliers\n",
    "dataset=dataset[(np.abs(stats.zscore(dataset)) < 9).all(axis=1)]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting data into train and test subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop('imdb_score', axis = 1)\n",
    "y = dataset['imdb_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(4, 4, figsize =[10,10])\n",
    "plt.tight_layout(pad=0.4, w_pad=1.0, h_pad=1.0)\n",
    "for n,col in enumerate(dataset.columns[0:16]):\n",
    "    sns.regplot(x=col, y=\"imdb_score\", data=dataset, ax=axes[n//4,n%4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(6, 4, figsize =[10,10])\n",
    "plt.tight_layout(pad=0.4, w_pad=1.0, h_pad=1.0)\n",
    "for n,col in enumerate(dataset.columns[16:]):\n",
    "    sns.barplot(x=col, y=\"imdb_score\", data=dataset, ax=axes[n//4,n%4])\n",
    "f.delaxes(axes[5,2]);f.delaxes(axes[5,3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: [sklearn docs](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart)\n",
    "\n",
    "_What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn?_\n",
    "\n",
    "**ID3** (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n",
    "\n",
    "**C4.5** is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.\n",
    "\n",
    "**C5.0** is Quinlan’s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.\n",
    "\n",
    "**CART** (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.\n",
    "\n",
    "**scikit-learn uses an optimised version of the CART algorithm**; however, scikit-learn implementation does not support categorical variables for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART algorithm pick variables and cutoff threshold using:\n",
    " 1. __for classification__: minimization of node's heterogeneity (Gini index or entropy) \n",
    " 2. __for regression__: minimizing error of prediction (e.g. sum of squares of residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CART = tree.DecisionTreeRegressor(random_state=42,ccp_alpha=0.0)\n",
    "CART_model=CART.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CART_model.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_model.get_n_leaves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruning CART tree (cost based)**\n",
    "\n",
    "⚠️ Requires scikit-learn >=0.22\n",
    "\n",
    "[Minimal Cost-Complexity Pruning](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = CART.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas[::10], path.impurities[::10]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = tree.DecisionTreeRegressor(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "      clfs[-1].tree_.node_count, ccp_alphas[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(model,X,y):\n",
    "    return np.sqrt(((model.predict(X)-y)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = [RMSE(clf,X_test,y_test) for clf in clfs]\n",
    "train_scores = [RMSE(clf,X_train,y_train) for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10,10])\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"RMSE vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complexity (cost) that produce the best regression tree\n",
    "Best_CART = clfs[np.argmin(test_scores)]\n",
    "Best_CART.ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !conda install python-graphviz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting printed tree to file\n",
    "import graphviz\n",
    "tree.export_graphviz(Best_CART, out_file=\"cart.dot\", \n",
    "                     feature_names=X_train.columns,\n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng cart.dot -o cart.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE of the best tree\n",
    "min(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#But we can view this as multiclass classification\n",
    "confmat = pd.crosstab(Best_CART.predict(X_test).round(),y_test.round())\n",
    "confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "np.array([confmat.loc[i,i] for i in confmat.index]).sum()/confmat.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble tree-based methods\n",
    "\n",
    "Ensemble learning helps improve final model performance by combining results of underlying models (e.g. random forest is combination of decision trees). This approach allows the production of better predictive performance compared to a single model.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/ensemble.html#\n",
    "\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "\n",
    "- In **averaging methods**, the driving principle is to build several estimators **independently** and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n",
    "\n",
    "> Example: Random forests\n",
    "\n",
    "- By contrast, in **boosting methods**, base estimators are built **sequentially** and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n",
    "\n",
    "> Example: Boosted trees\n",
    "\n",
    "<img src=\"https://hpccsystems.com/sites/default/files/inline-images/LearningTrees.PNG\" width=400>\n",
    "\n",
    "[Source](https://hpccsystems.com/blog/learning-trees-guide-to-decision-tree-based-machine-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forests**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/ensemble.html#random-forests\n",
    "\n",
    "[sklearn.ensemble.RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)\n",
    "\n",
    "We'll look on two parameters:\n",
    "- n_estimators - number of trees built in ensemble\n",
    "- max_features - how many features will be included in each tree e.g.\n",
    "    - 'auto' - all\n",
    "    - 'sqrt' - random sample of sqrt(n_features)\n",
    "    - n - random sample of 'n' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking number of tress influence on RMSE\n",
    "rfr = RandomForestRegressor\n",
    "N = [10,50,100,200,300,400,500]\n",
    "RMSE_RF= [RMSE(rfr(n,n_jobs=-1).fit(X_train,y_train),X_test,y_test) for n in N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(N,RMSE_RF,'.-',color='g');\n",
    "N[np.argmin(RMSE_RF)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking number of features influence on RMSE\n",
    "features = np.linspace(1,X_train.shape[1],10).astype(int)\n",
    "RMSE_RF_features= [RMSE(rfr(400,max_features=n,n_jobs=-1).fit(X_train,y_train),X_test,y_test) for n in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(features,RMSE_RF_features,'.-',color='r');\n",
    "features[np.argmin(RMSE_RF_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_RF = RandomForestRegressor(400,max_features=25,n_jobs=-1).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "importances = Best_RF.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in Best_RF.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "num_feat = 6\n",
    "plt.figure(figsize=[10,5])\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(num_feat)[:num_feat], importances[indices][:num_feat],\n",
    "       color=\"r\", yerr=std[indices][:num_feat], align=\"center\")\n",
    "plt.xticks(range(num_feat)[:num_feat], X_train.columns[indices])\n",
    "plt.xlim([-1, num_feat])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosted Trees**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking number of tress influence on RMSE\n",
    "gbr = GradientBoostingRegressor\n",
    "N = [10,50,100,200,300,400,500,600,700]\n",
    "RMSE_GBT = [RMSE(gbr(n_estimators=n).fit(X_train,y_train),X_test,y_test) for n in N]\n",
    "\n",
    "plt.plot(N,RMSE_GBT,'.-',color='y');\n",
    "N[np.argmin(RMSE_GBT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From 500 trees RMSE reduction is insignificant\n",
    "Best_GBT = GradientBoostingRegressor(n_estimators=500).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of deviance between training and test set\n",
    "test_score = np.zeros((500,), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(Best_GBT.staged_predict(X_test)):\n",
    "    test_score[i] = Best_GBT.loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(500) + 1, Best_GBT.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(500) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "feature_importance = Best_GBT.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "num_feat = 6\n",
    "plt.figure(figsize=[10,5])\n",
    "plt.barh(pos[-num_feat:], feature_importance[sorted_idx][-num_feat:], align='center')\n",
    "plt.yticks(pos[-num_feat:], X_train.columns[sorted_idx][-num_feat:])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing results of Decision Tree, Random Forest and Gradient Boosted Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Best_CART, Best_RF, Best_GBT]\n",
    "errors = [RMSE(m, X_test, y_test) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(['CART','Random Forest','Gradient Boosted Trees'], errors, color=['red', 'green', 'blue'], alpha=0.75);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
